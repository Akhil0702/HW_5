Student: Vancha Akhil Reddy
Student ID: 700772768


ğŸ“Œ Overview

This assignment contains two coding problems related to core components of the Transformer architecture:

Q1: Implementing Scaled Dot-Product Attention using NumPy

Q2: Implementing a simplified Transformer Encoder Block using PyTorch

Each question includes function implementations and optional test code to verify correctness.

âœ… Q1 â€“ Scaled Dot-Product Attention (NumPy)
Files Included

Q1.py

Description

In this task, I implemented the Scaled Dot-Product Attention mechanism as defined in the "Attention Is All You Need" paper.

The function performs:

Compute raw attention scores using matrix multiplication:

scores
=
ğ‘„
ğ¾
âŠ¤
scores=QK
âŠ¤

Scale scores by 
ğ‘‘
ğ‘˜
d
k
	â€‹

	â€‹


Apply a numerically stable softmax

Compute the context vector:

context
=
softmax(scores)
â‹…
ğ‘‰
context=softmax(scores)â‹…V
Functions Implemented
âœ” softmax(x, axis=-1)

A numerically stable softmax that subtracts the max value to avoid overflow.

âœ” scaled_dot_product_attention(Q, K, V)

Returns:

attn_weights: attention weights

context: the resulting context vectors

How to Run
python Q1.py


If the test block is enabled, the script will print:

Attention weights shape: (seq_len_q, seq_len_k)

Context shape: (seq_len_q, d_v)

âœ… Q2 â€“ Transformer Encoder Block (PyTorch)
Files Included

Q2.py

Description

This task implements a simplified version of the Transformer Encoder Block, including:

Multi-Head Self-Attention

Feed-Forward Network (FFN)

Residual Connections

Layer Normalization

Dropout (optional)

This follows the structure of the original Transformer model.

Class Implemented
âœ” SimpleTransformerEncoderBlock(d_model, num_heads, d_ff, dropout)

The forward(x) method performs:

Multi-Head Self-Attention

Add & Norm

Feed-Forward Network

Add & Norm again

How to Run

Make sure PyTorch is installed in the environment.

python Q2.py


If the test block is included, it will print:

Input shape : torch.Size([32, 10, d_model])
Output shape: torch.Size([32, 10, d_model])


confirming the encoder block returns output in the same shape as the input.
